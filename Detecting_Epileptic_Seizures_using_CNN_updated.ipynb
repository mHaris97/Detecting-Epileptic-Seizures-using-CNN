{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting Epileptic Seizures using CNN_updated.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-2zlps5gB-V1",
        "KKermRpjCq5J",
        "VOXRJRMUDhZs",
        "WQZLzVtIfNqr",
        "9fr-O3J-RAGc",
        "OwOnh6gHD2Wg",
        "gvGB-qiKrher"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Lseco7C3ES"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2zlps5gB-V1"
      },
      "source": [
        "## Setting up Tensorflow\n",
        "Restart runtime, set it to TPU and then run this chunk. The default is tf2.x which has slightly different functions. I prefer working with tf1.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rdi5jiOCWmY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9911740-eec3-4457-96bc-7da6c68e4dd2"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKermRpjCq5J"
      },
      "source": [
        "## Loading Dataset\n",
        "Loading the EEG dataset from physionet databases [1]. Physionet allows us to load dataset directly from cloud. \n",
        "**Stop as soon as 24th folder has been loaded, try not to go below 20 GB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZggaaYKDMtM"
      },
      "source": [
        "# !wget -r -N -c -np https://physionet.org/files/chbmit/1.0.0/\n",
        "!gsutil -m cp -r gs://chbmit-1.0.0.physionet.org DESTINATION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3NCs89XrurD"
      },
      "source": [
        "Checking some files content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWetP4mbrIUt"
      },
      "source": [
        "#Just to see if files got loaded.\n",
        "file = '/content/DESTINATION/chbmit-1.0.0.physionet.org/chb03/chb03-summary.txt'\n",
        "f = open(file, 'r')\n",
        "file_contents = f.read()ON\n",
        "print(file_contents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT5Ih52msbCh"
      },
      "source": [
        "# only run this for the first time.\n",
        "!pip install pyedflib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnXYN6yIr0md"
      },
      "source": [
        "# Just to check if things are syncing in.\n",
        "from pyedflib import highlevel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read an edf file\n",
        "file2 = '/content/DESTINATION/chbmit-1.0.0.physionet.org/chb02/chb02_16+.edf'\n",
        "signals, signal_headers, header = highlevel.read_edf(file2)\n",
        "\n",
        "dft = np.fft.fft(signals, axis=1)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.transpose(signals[0:3,:1000]))\n",
        "plt.title('Raw signals')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.transpose(dft[0:3,:1000]))\n",
        "plt.title('Fourier transform')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOXRJRMUDhZs"
      },
      "source": [
        "## Preproccessing\n",
        "Some basic data preprocessing includes obtaining signals in frequency domain usinf fft and shaping the data to arrange as labels and training+test data. Run all of this. Very important chunk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQZLzVtIfNqr"
      },
      "source": [
        "### First we will read all the \".edf\" and \".txt\" files in the directory and stack them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHS0g7cSD1w3"
      },
      "source": [
        "import os\n",
        "\n",
        "path = '/content/DESTINATION'\n",
        "\n",
        "edfFiles = []\n",
        "txtFiles = []\n",
        "# r=root, d=directories, f = files\n",
        "for r, d, f in os.walk(path):\n",
        "    for file in f:\n",
        "        if file[-4:] == '.edf':\n",
        "            edfFiles.append(os.path.join(r, file))\n",
        "        elif file[-4:] == '.txt':\n",
        "            txtFiles.append(os.path.join(r, file))\n",
        "\n",
        "edfFiles = sorted(edfFiles)\n",
        "txtFiles = sorted(txtFiles)\n",
        "\n",
        "for f in edfFiles:\n",
        "    print(f)\n",
        "\n",
        "for f in txtFiles:\n",
        "    print(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BruhxyL2hYMH"
      },
      "source": [
        "### Reading EDF & TXT files and stacking them in batches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdLCEu3fhaEK"
      },
      "source": [
        "from pyedflib import highlevel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generateLabels(edfFileName):\n",
        "  sub = edfFileName[54:59]\n",
        "  filePath = '/content/DESTINATION/chbmit-1.0.0.physionet.org/' + sub + '/' + sub + '-summary.txt'\n",
        "  f = open(filePath, 'r')\n",
        "  file_contents = f.read()\n",
        "\n",
        "  file_list = file_contents.split('\\n')\n",
        "  sub = edfFileName[54:-4]\n",
        "  sub = 'File Name: ' + sub + '.edf'\n",
        "  ind = file_list.index(sub)\n",
        "\n",
        "  seizures = list(map(int, re.findall(r'\\d+', file_list[ind+3]) ))[0]\n",
        "  start = []\n",
        "  end   = []\n",
        "  for i in range(seizures):\n",
        "    start.append(list(map(int, re.findall(r'\\d+', file_list[ind+2*i+4])))[0])\n",
        "    end.append(list(map(int, re.findall(r'\\d+', file_list[ind+2*i+5])))[0])\n",
        "    # print(start, end)\n",
        "\n",
        "  if seizures == 0:\n",
        "    labels = np.zeros((3600))\n",
        "  else:\n",
        "    labels = np.ones((3600))\n",
        "    labels[end[-1]:] *= 0\n",
        "    for i in range(len(start)):\n",
        "      labels[start[i]:end[i]] *= 2\n",
        "  \n",
        "  return labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWDuOlnJ4y5L"
      },
      "source": [
        "Shuffling and partitioning list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzx0zzwaz6FR"
      },
      "source": [
        "import random\n",
        "\n",
        "totalData       = len(edfFiles)\n",
        "# random.shuffle(edfFiles)\n",
        "partition       = int(len(edfFiles) * 2/3)\n",
        "edfFilesVal     = edfFiles[partition:]\n",
        "edfFilesTrain   = edfFiles[:partition]\n",
        "trainData       = len(edfFilesTrain)\n",
        "valData         = len(edfFilesVal)\n",
        "\n",
        "print(totalData, trainData, valData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umqviX-bK27k"
      },
      "source": [
        "Frequency Domain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng5d6ZdaK4j7"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "def stackDFTTrain(nbatch = 2):\n",
        "  count = 0\n",
        "\n",
        "  stackedDFT = np.zeros((1, 23, 256, 3))\n",
        "  stackedLabels = np.zeros((1))\n",
        "  rejected   = []\n",
        "\n",
        "  while True:\n",
        "    for f in edfFilesTrain:\n",
        "      # print(f[54:-4])\n",
        "      if stackedDFT.shape[0] >= nbatch*3600//3 + 1:\n",
        "        print(stackedLabels.shape)\n",
        "        if stackedDFT[1:nbatch*3600//3 + 1,:,:,:].shape == (3600*nbatch//3, 23, 256, 3) and to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3).shape == (3600*nbatch//3, 3):\n",
        "          yield (stackedDFT[1:nbatch*3600//3 + 1,:,:,:],\n",
        "                 to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3))\n",
        "        stackedDFT = stackedDFT[nbatch*3600//3:,:,:,:]\n",
        "        stackedLabels = stackedLabels[nbatch*3600:]\n",
        "        print('extra', stackedDFT.shape, stackedLabels.shape)\n",
        "\n",
        "      signals, signal_headers, header = highlevel.read_edf(f)\n",
        "      if signals.shape[-1] % 3600 != 0 or signals.shape[0] != 23:\n",
        "        rejected.append(f[54:59])\n",
        "        continue\n",
        "      \n",
        "      # if signals.shape != (23, 921600):\n",
        "      #   rejected.append(f[54:59])\n",
        "      #   continue\n",
        "\n",
        "      count += 1\n",
        "      print(f, signals.shape)\n",
        "      s = int(signals.shape[1]/256)\n",
        "      signals = np.reshape(signals, (23,256,3,s//3))\n",
        "      signals = signals.transpose(3,0,1,2)\n",
        "      stackedDFT = np.concatenate((stackedDFT, np.fft.fft(signals, axis=1)), axis=0)\n",
        "      genLabels = generateLabels(f)\n",
        "      stackedLabels = np.concatenate((stackedLabels, genLabels), axis=-1)\n",
        "    \n",
        "    \n",
        "\n",
        "def stackDFTVal(nbatch = 1):\n",
        "  count = 0\n",
        "\n",
        "  stackedDFT = np.zeros((1, 23, 256, 3))\n",
        "  stackedLabels = np.zeros((1))\n",
        "  rejected   = []\n",
        "  \n",
        "  while True:\n",
        "\n",
        "    for f in edfFilesVal:\n",
        "      # print(f[54:-4])\n",
        "      if stackedDFT.shape[0] >= nbatch*3600//3 + 1:\n",
        "        # stackedDFT = np.reshape(stackedDFT, (1,)+stackedDFT.shape)\n",
        "        # stackedDFT = np.reshape(stackedDFT, (1,23,256,3600\n",
        "        # yield (np.reshape(stackedDFT[1:nbatch*3600//3 + 1,:,:,:], \n",
        "        #                   (1,)+stackedDFT[1:nbatch*3600//3 + 1,:,:,:].shape),\n",
        "        #        to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3))\n",
        "        if stackedDFT[1:nbatch*3600//3 + 1,:,:,:].shape == (3600*nbatch//3, 23, 256, 3) and to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3).shape == (3600*nbatch//3, 3):\n",
        "          yield (stackedDFT[1:nbatch*3600//3 + 1,:,:,:],\n",
        "                 to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3))\n",
        "        stackedDFT = stackedDFT[nbatch*3600//3:,:,:,:]\n",
        "        stackedLabels = stackedLabels[nbatch*3600:]\n",
        "\n",
        "      signals, signal_headers, header = highlevel.read_edf(f)\n",
        "      if signals.shape[-1] % 3600 != 0 or signals.shape[0] != 23:\n",
        "        rejected.append(f[54:59])\n",
        "        continue\n",
        "\n",
        "      # if signals.shape != (23, 921600):\n",
        "      #   rejected.append(f[54:59])\n",
        "      #   continue\n",
        "      count += 1\n",
        "      print(f, signals.shape)\n",
        "      s = int(signals.shape[1]/256)\n",
        "      signals = np.reshape(signals, (23,256,3,s//3))\n",
        "      signals = signals.transpose(3,0,1,2)\n",
        "      stackedDFT = np.concatenate((stackedDFT, np.fft.fft(signals, axis=1)), axis=0)\n",
        "      genLabels = generateLabels(f)\n",
        "      stackedLabels = np.concatenate((stackedLabels, genLabels), axis=-1)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# for h in stackDFTVal():\n",
        "#   print(len(h), h[0].shape, h[1].shape)\n",
        "#   break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YVGVgTEDlNc"
      },
      "source": [
        "# for i in range(73):\n",
        "#   for h in stackDFTTrain():\n",
        "#     print(len(h), h[0].shape, h[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQotIf-3Kuso"
      },
      "source": [
        "Time domain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcP28kbiAIPq"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "def stackTimeTrain(nbatch = 2):\n",
        "  count = 0\n",
        "\n",
        "  stackedDFT = np.zeros((1, 23, 256, 3))\n",
        "  stackedLabels = np.zeros((1))\n",
        "  rejected   = []\n",
        "\n",
        "  while True:\n",
        "    for f in edfFilesTrain:\n",
        "      # print(f[54:-4])\n",
        "      if stackedDFT.shape[0] >= nbatch*3600//3 + 1:\n",
        "        print(stackedLabels.shape)\n",
        "        if stackedDFT[1:nbatch*3600//3 + 1,:,:,:].shape == (3600*nbatch//3, 23, 256, 3) and to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3).shape == (3600*nbatch//3, 3):\n",
        "          yield (stackedDFT[1:nbatch*3600//3 + 1,:,:,:],\n",
        "                 to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3))\n",
        "        stackedDFT = stackedDFT[nbatch*3600//3:,:,:,:]\n",
        "        stackedLabels = stackedLabels[nbatch*3600:]\n",
        "        print('extra', stackedDFT.shape, stackedLabels.shape)\n",
        "\n",
        "      signals, signal_headers, header = highlevel.read_edf(f)\n",
        "      if signals.shape[-1] % 3600 != 0 or signals.shape[0] != 23:\n",
        "        rejected.append(f[54:59])\n",
        "        continue\n",
        "      \n",
        "      # if signals.shape != (23, 921600):\n",
        "      #   rejected.append(f[54:59])\n",
        "      #   continue\n",
        "\n",
        "      count += 1\n",
        "      print(f, signals.shape)\n",
        "      s = int(signals.shape[1]/256)\n",
        "      signals = np.reshape(signals, (23,256,3,s//3))\n",
        "      signals = signals.transpose(3,0,1,2)\n",
        "      stackedDFT = np.concatenate((stackedDFT, signals), axis=0)\n",
        "      genLabels = generateLabels(f)\n",
        "      stackedLabels = np.concatenate((stackedLabels, genLabels), axis=-1)\n",
        "    \n",
        "    \n",
        "\n",
        "def stackTimeVal(nbatch = 1):\n",
        "  count = 0\n",
        "\n",
        "  stackedDFT = np.zeros((1, 23, 256, 3))\n",
        "  stackedLabels = np.zeros((1))\n",
        "  rejected   = []\n",
        "\n",
        "  while True:\n",
        "    for f in edfFilesTrain:\n",
        "      # print(f[54:-4])\n",
        "      if stackedDFT.shape[0] >= nbatch*3600//3 + 1:\n",
        "        print(stackedLabels.shape)\n",
        "        if stackedDFT[1:nbatch*3600//3 + 1,:,:,:].shape == (3600*nbatch//3, 23, 256, 3) and to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3).shape == (3600*nbatch//3, 3):\n",
        "          yield (stackedDFT[1:nbatch*3600//3 + 1,:,:,:],\n",
        "                 to_categorical(stackedLabels[1:nbatch*3600//3 + 1], num_classes=3))\n",
        "        stackedDFT = stackedDFT[nbatch*3600//3:,:,:,:]\n",
        "        stackedLabels = stackedLabels[nbatch*3600:]\n",
        "        print('extra', stackedDFT.shape, stackedLabels.shape)\n",
        "\n",
        "      signals, signal_headers, header = highlevel.read_edf(f)\n",
        "      if signals.shape[-1] % 3600 != 0 or signals.shape[0] != 23:\n",
        "        rejected.append(f[54:59])\n",
        "        continue\n",
        "      \n",
        "      # if signals.shape != (23, 921600):\n",
        "      #   rejected.append(f[54:59])\n",
        "      #   continue\n",
        "\n",
        "      count += 1\n",
        "      print(f, signals.shape)\n",
        "      s = int(signals.shape[1]/256)\n",
        "      signals = np.reshape(signals, (23,256,3,s//3))\n",
        "      signals = signals.transpose(3,0,1,2)\n",
        "      stackedDFT = np.concatenate((stackedDFT, signals), axis=0)\n",
        "      genLabels = generateLabels(f)\n",
        "      stackedLabels = np.concatenate((stackedLabels, genLabels), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fr-O3J-RAGc"
      },
      "source": [
        "## Callback Class\n",
        "In order to stop the training at a given threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clPdvxcld3XW"
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "\n",
        "# when accuracy reaches ACCURACY_THRESHOLD\n",
        "ACCURACY_THRESHOLD = 0.95\n",
        "\n",
        "class myCallback(Callback):\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\tif(logs.get('acc') > ACCURACY_THRESHOLD):\n",
        "\t\t\tprint(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))\n",
        "\t\t\tself.model.stop_training = True\n",
        "\n",
        "# Instantiate a callback object\n",
        "callbacks = myCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwOnh6gHD2Wg"
      },
      "source": [
        "## Building a CNN for Frequency domain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kuWlEWcE579"
      },
      "source": [
        "Here we just import some libraries and use them to buid an architecture. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYhHULJ0EHm1"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, AveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "nBatch = 2\n",
        "\n",
        "in1 = Input(shape=(23, 256, 3))\n",
        "c1 = Conv2D(16, (5,5), activation='relu')(in1)\n",
        "# m1 = MaxPooling2D()(c1)\n",
        "m1 = AveragePooling2D()(c1)\n",
        "# c2 = Conv2D(128, (5,5), activation='relu')(m1)\n",
        "# # m2 = MaxPooling2D()(c2)\n",
        "# d1 = Dropout(0.7)(c2)\n",
        "# c3 = Conv2D(256, (5,5), activation='relu')(d1)\n",
        "# # m3 = MaxPooling2D()(c3)\n",
        "# d2 = Dropout(0.8)(c3)\n",
        "# # c4 = Conv2D(64, (3,3), activation='relu')(d2)\n",
        "# # m4 = MaxPooling2D()(c4)\n",
        "# fl = Flatten()(d2)\n",
        "# d1 = Dense(64, activation='relu')(fl)\n",
        "# d2 = Dense(16, activation='relu')(d1)\n",
        "# o = Dense(921600*nBatch, activation='softmax')(m1)\n",
        "fl = Flatten()(m1)\n",
        "# d1 = Dense(4, activation='relu')(fl)\n",
        "o = Dense(3, activation='sigmoid')(fl)\n",
        "\n",
        "model = Model(inputs=in1, outputs=o)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yOu7xspFav5"
      },
      "source": [
        "Now after we have constructed our model let's train it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae6_YvbpFiRj"
      },
      "source": [
        "model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics=['acc'])\n",
        "# stepsTrain = int(len(edfFiles)/nBatch * 11/15)\n",
        "# stepsVal = int(len(edfFiles)/nBatch * 4/15)\n",
        "testSteps = int(trainData/(16*8))\n",
        "valSteps = int(valData/16)\n",
        "history_cnn = model.fit_generator(generator = stackDFTTrain(), \n",
        "                                  steps_per_epoch = testSteps, \n",
        "                                  epochs = 7, \n",
        "                                  validation_data = stackDFTVal(), \n",
        "                                  validation_steps = valSteps)\n",
        "                                  # callbacks=[callbacks])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KapMK5FJEOml"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6W3Omw4EI2F"
      },
      "source": [
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "loss = history_cnn.history['loss']\n",
        "val_loss = history_cnn.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "pyplot.grid()\n",
        "pyplot.plot(epochs, loss, '*y-', label='Training loss')\n",
        "pyplot.plot(epochs, val_loss, '*r-', label='Validation loss')\n",
        "pyplot.title('Training and validation loss')\n",
        "pyplot.xlabel('Epochs')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.grid()\n",
        "acc = history_cnn.history['acc']\n",
        "val_acc = history_cnn.history['val_acc']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "pyplot.plot(epochs, acc, '*y-', label='Training Accuracy')\n",
        "pyplot.plot(epochs, val_acc, '*r-', label='Validation Accuracy')\n",
        "pyplot.title('Training and validation Accuracies')\n",
        "pyplot.xlabel('Epochs')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bav6DSIkBCLk"
      },
      "source": [
        "history_cnn.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvGB-qiKrher"
      },
      "source": [
        "## CNN for Time domain\n",
        "\n",
        "Repeating the process for Time domain, with the same parameters and architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9-OK7iTr0e9"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, AveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "nBatch = 2\n",
        "\n",
        "in1 = Input(shape=(23, 256, 3))\n",
        "c1 = Conv2D(16, (5,5), activation='relu')(in1)\n",
        "m1 = MaxPooling2D()(c1)\n",
        "# m1 = AveragePooling2D()(c1)\n",
        "# c2 = Conv2D(128, (5,5), activation='relu')(m1)\n",
        "# # m2 = MaxPooling2D()(c2)\n",
        "# d1 = Dropout(0.7)(c2)\n",
        "# c3 = Conv2D(256, (5,5), activation='relu')(d1)\n",
        "# # m3 = MaxPooling2D()(c3)\n",
        "# d2 = Dropout(0.8)(c3)\n",
        "# # c4 = Conv2D(64, (3,3), activation='relu')(d2)\n",
        "# # m4 = MaxPooling2D()(c4)\n",
        "# fl = Flatten()(d2)\n",
        "# d1 = Dense(64, activation='relu')(fl)\n",
        "# d2 = Dense(16, activation='relu')(d1)\n",
        "# o = Dense(921600*nBatch, activation='softmax')(m1)\n",
        "fl = Flatten()(m1)\n",
        "# d1 = Dense(4, activation='relu')(fl)\n",
        "o = Dense(3, activation='sigmoid')(fl)\n",
        "\n",
        "model2 = Model(inputs=in1, outputs=o)\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTWhj-Egr-nc"
      },
      "source": [
        "model2.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics=['acc'])\n",
        "# stepsTrain = int(len(edfFiles)/nBatch * 11/15)\n",
        "# stepsVal = int(len(edfFiles)/nBatch * 4/15)\n",
        "testSteps = int(trainData/(16*8))\n",
        "valSteps = int(valData/8)\n",
        "history_cnn2 = model2.fit_generator(generator = stackTimeTrain(),\n",
        "                                    steps_per_epoch = testSteps, \n",
        "                                    epochs = 7, \n",
        "                                    validation_data = stackTimeVal(), \n",
        "                                    validation_steps = valSteps)\n",
        "                                    # callbacks=[callbacks])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdaDH80msI01"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbS3xMNqsIBs"
      },
      "source": [
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "loss2 = history_cnn2.history['loss']\n",
        "val_loss2 = history_cnn2.history['val_loss']\n",
        "epochs2 = range(1, len(loss2) + 1)\n",
        "pyplot.grid()\n",
        "pyplot.plot(epochs2, loss2, '*y-', label='Training loss')\n",
        "pyplot.plot(epochs2, val_loss2, '*r-', label='Validation loss')\n",
        "pyplot.title('Training and validation loss')\n",
        "pyplot.xlabel('Epochs')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.grid()\n",
        "acc2 = history_cnn2.history['acc']\n",
        "val_acc2 = history_cnn2.history['val_acc']\n",
        "epochs2 = range(1, len(loss2) + 1)\n",
        "pyplot.plot(epochs2, acc2, '*y-', label='Training Accuracy')\n",
        "pyplot.plot(epochs2, val_acc2, '*r-', label='Validation Accuracy')\n",
        "pyplot.title('Training and validation Accuracies')\n",
        "pyplot.xlabel('Epochs')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKCAOecTCyuv"
      },
      "source": [
        "# Bibliography\n",
        "[1] CHB-MIT Scalp EEG Database, Retrieved from: https://physionet.org/content/chbmit/1.0.0/\n"
      ]
    }
  ]
}